{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d7dd42e-ad7c-4691-9323-58f3022c34e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Objective - \n",
    "Create generation on a model for human eval data, evaluate the generations and write them to a file\n",
    "\"\"\"\n",
    "\n",
    "model_path = \"Meta-Llama-3-8B-Instruct-oss-full-2e-4bs\" # local path/s3 name/huggingface name of model\n",
    "model_name = \"Meta-Llama-3-8B-Instruct-oss-full-2e-4bs\" # model name (DONT keep any '/' or '.' in there)\n",
    "tokeniser = None # huggingface name/local path of tokeniser, make it None if model already has tokeniser\n",
    "isS3 = True # true if model is to be loaded from s3, make sure ./model directory is empty\n",
    "isHF = False  # true if model is to be loaded from huggingface\n",
    "isLocal = False # true if model is to be loaded from local\n",
    "benchmark='humaneval' #choose from [\"humaneval\", \"mbpp\"]\n",
    "home_path = '/home/ec2-user/SageMaker'   # local path starting from root to CodeLLMRnD\n",
    "\n",
    "run_generations = True # make it false if you only want to evaluate\n",
    "greedy = True # evalplus -> greedy True, if want to give custom n_samples, temperature, max_length_generation, top_p, pass greety=True\n",
    "n_samples = 50 # evalplus -> n_samples=1, bigcode -> n_samples=50\n",
    "temperature = 0.2 # evalplus -> temperature=0, bigcode -> temperature=0.2\n",
    "\n",
    "merged_results_file_name= \"evalplusrepo_\"+benchmark+\"_results_\"+model_name+\".jsonl\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4fee2a-8c12-446a-abb9-55f08cff9d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/ec2-user/SageMaker/CodeLLMRnD/evalplus\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: wget>=3.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evalplus==0.1.0.dev699) (3.2)\n",
      "Requirement already satisfied: tempdir>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evalplus==0.1.0.dev699) (0.7.1)\n",
      "Requirement already satisfied: multipledispatch>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evalplus==0.1.0.dev699) (1.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evalplus==0.1.0.dev699) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evalplus==0.1.0.dev699) (1.22.4)\n",
      "Requirement already satisfied: tqdm>=4.56.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evalplus==0.1.0.dev699) (4.66.2)\n",
      "Requirement already satisfied: termcolor>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evalplus==0.1.0.dev699) (2.4.0)\n",
      "Requirement already satisfied: fire>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evalplus==0.1.0.dev699) (0.6.0)\n",
      "Requirement already satisfied: openai>=1.11.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evalplus==0.1.0.dev699) (1.23.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fire>=0.6.0->evalplus==0.1.0.dev699) (1.16.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai>=1.11.1->evalplus==0.1.0.dev699) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai>=1.11.1->evalplus==0.1.0.dev699) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai>=1.11.1->evalplus==0.1.0.dev699) (0.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai>=1.11.1->evalplus==0.1.0.dev699) (2.7.0)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai>=1.11.1->evalplus==0.1.0.dev699) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai>=1.11.1->evalplus==0.1.0.dev699) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.11.1->evalplus==0.1.0.dev699) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.11.1->evalplus==0.1.0.dev699) (1.2.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.11.1->evalplus==0.1.0.dev699) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.11.1->evalplus==0.1.0.dev699) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.11.1->evalplus==0.1.0.dev699) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai>=1.11.1->evalplus==0.1.0.dev699) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai>=1.11.1->evalplus==0.1.0.dev699) (2.18.1)\n",
      "Building wheels for collected packages: evalplus\n",
      "  Building editable for evalplus (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for evalplus: filename=evalplus-0.1.0.dev699-0.editable-py3-none-any.whl size=12239 sha256=634fa8d7ba00440f5e3651dc69fda2cad29163d79a6f7ca9817e9f2f38b0ebeb\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-401pw8oi/wheels/73/d0/bb/c6ae10827b44a29c868345417fd245beb475210ed86aaf5cf8\n",
      "Successfully built evalplus\n",
      "Installing collected packages: evalplus\n",
      "  Attempting uninstall: evalplus\n",
      "    Found existing installation: evalplus 0.3.0.dev25\n",
      "    Uninstalling evalplus-0.3.0.dev25:\n",
      "      Successfully uninstalled evalplus-0.3.0.dev25\n",
      "Successfully installed evalplus-0.1.0.dev699\n"
     ]
    }
   ],
   "source": [
    "!pip install -e .\n",
    "!pip install -q -r requirements.txt\n",
    "!pip install -q -r codegen/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b62e946-d3db-4cf1-a3d9-027d6cef3d54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_results_dir = benchmark + \"_inference/\" # local path to save results - end the path with a \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a15336d4-cf9d-4dd0-8b4e-a1815115ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../human_eval')\n",
    "from run_bash_command import run_bash_command\n",
    "if not isLocal and run_generations:\n",
    "    run_bash_command(f\"sudo find {home_path} -type f -size +500M -print0 | xargs -0 rm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97d12741-2ed8-4e89-b33a-c2bfd4c313c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "if os.path.isdir(inference_results_dir) and run_generations:\n",
    "    shutil.rmtree(inference_results_dir)\n",
    "os.makedirs(os.path.dirname(inference_results_dir), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4df64458-d999-4b0c-929e-1eab1f9ef253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if isS3:\n",
    "    sys.path.append('../fine_tuning/utils')\n",
    "    from modelComm import ModelComm\n",
    "    modelComm = ModelComm()\n",
    "    model_path = modelComm.download_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1972265d-3ccf-40a8-bf44-928ca5695c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python codegen/generate.py --model ./model/Meta-Llama-3-8B-Instruct-oss-full-2e-4bs --bs 1 --greedy --root humaneval_inference/ --dataset humaneval --local\n",
      "There was a problem when trying to write in your cache folder (/JawTitan/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.\n",
      "Initializing a decoder model: ./model/Meta-Llama-3-8B-Instruct-oss-full-2e-4bs ...\n",
      "INFO 04-19 21:18:52 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='./model/Meta-Llama-3-8B-Instruct-oss-full-2e-4bs', tokenizer='./model/Meta-Llama-3-8B-Instruct-oss-full-2e-4bs', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO 04-19 21:18:55 selector.py:51] Cannot use FlashAttention because the package is not found. Please install it for better performance.\n",
      "INFO 04-19 21:18:55 selector.py:25] Using XFormers backend.\n",
      "INFO 04-19 21:19:01 model_runner.py:104] Loading model weights took 14.9595 GB\n",
      "INFO 04-19 21:19:02 gpu_executor.py:94] # GPU blocks: 9788, # CPU blocks: 2048\n",
      "INFO 04-19 21:19:04 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-19 21:19:04 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-19 21:19:08 model_runner.py:867] Graph capturing finished in 4 secs.\n",
      "\u001b[2KCodegen: HumanEval_0 @ .\u001b[35m/model/\u001b[0m\u001b[95mMeta-Llama-3-8B-Instruct-oss-full-2e-4bs\u001b[0m0m • \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[2KCodegen: HumanEval_1 @ .\u001b[35m/model/\u001b[0m\u001b[95mMeta-Llama-3-8B-Instruct-oss-full-2e-4bs\u001b[0m0m • \u001b[33m0:00:01\u001b[0m\n",
      "\u001b[2KCodegen: HumanEval_2 @ .\u001b[35m/model/\u001b[0m\u001b[95mMeta-Llama-3-8B-Instruct-oss-full-2e-4bs\u001b[0m0m • \u001b[33m0:00:08\u001b[0m\n",
      "\u001b[2KCodegen: HumanEval_3 @ .\u001b[35m/model/\u001b[0m\u001b[95mMeta-Llama-3-8B-Instruct-oss-full-2e-4bs\u001b[0m0m • \u001b[33m0:00:09\u001b[0m\n",
      "\u001b[2KCodegen: HumanEval_4 @ .\u001b[35m/model/\u001b[0m\u001b[95mMeta-Llama-3-8B-Instruct-oss-full-2e-4bs\u001b[0m  3/164\u001b[0m • \u001b[33m0:00:11\u001b[0m\n",
      "\u001b[2KCodegen: HumanEval_5 @ .\u001b[35m/model/\u001b[0m\u001b[95mMeta-Llama-3-8B-Instruct-oss-full-2e-4bs\u001b[0m  4/164\u001b[0m • \u001b[33m0:00:13\u001b[0m\n",
      "\u001b[2KCodegen: HumanEval_6 @ .\u001b[35m/model/\u001b[0m\u001b[95mMeta-Llama-3-8B-Instruct-oss-full-2e-4bs\u001b[0m[0m \u001b[32m  5/164\u001b[0m • \u001b[33m0:00:14\u001b[0m\n",
      "\u001b[2KCodegen: HumanEval_7 @ .\u001b[35m/model/\u001b[0m\u001b[95mMeta-Llama-3-8B-Instruct-oss-full-2e-4bs\u001b[0m[0m \u001b[32m  6/164\u001b[0m • \u001b[33m0:00:21\u001b[0m\n",
      "\u001b[2KCodegen: HumanEval_8 @ .\u001b[35m/model/\u001b[0m\u001b[95mMeta-Llama-3-8B-Instruct-oss-full-2e-4bs\u001b[0m[0m \u001b[32m  7/164\u001b[0m • \u001b[33m0:00:22\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "local = (isHF==False)\n",
    "if greedy:\n",
    "    command = f\"python codegen/generate.py --model {model_path} --bs 1 --greedy --root {inference_results_dir} --dataset {benchmark}\"\n",
    "else:\n",
    "    command = f\"python codegen/generate.py --model {model_path} --bs 1 --temperature {temperature} --n_samples {n_samples} --root {inference_results_dir} --dataset {benchmark}\"\n",
    "\n",
    "if local:\n",
    "    command += \" --local\"\n",
    "print(command)\n",
    "if run_generations:\n",
    "    run_bash_command(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5955b1b-8e3a-4913-bbec-715d77e0ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "if greedy:\n",
    "    temperature=0.0\n",
    "command = f\"evalplus.evaluate --dataset {benchmark} --samples {inference_results_dir}{benchmark}/{model_path}_temp_{temperature}/ --i-just-wanna-run\"\n",
    "print(command)\n",
    "run_bash_command(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9360e7c3-ff06-4b1c-86a8-cb0d20f69b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if benchmark == \"mbpp\":\n",
    "    test_file = os.path.join('data','MbppPlus-v0.1.0.jsonl')\n",
    "else:\n",
    "    test_file = os.path.join('data','HumanEvalPlus-v0.1.9.jsonl')\n",
    "command = f\"python merge_results.py {inference_results_dir}{benchmark}/{model_path}_temp_0.0/eval_results.json {test_file} {inference_results_dir}\"\n",
    "print(command)\n",
    "run_bash_command(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb82704-8e1d-44f4-8f92-90494a815db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results_path = os.path.join(inference_results_dir,'final_results.jsonl')\n",
    "import sys; sys.path.append(home_path+'/CodeLLMRnD/fine_tuning/utils')\n",
    "from resultsComm import ResultsComm\n",
    "resultsComm = ResultsComm()\n",
    "resultsComm.upload_results(merged_results_path, merged_results_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
